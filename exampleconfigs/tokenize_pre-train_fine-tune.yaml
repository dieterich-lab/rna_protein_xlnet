#
# Below is the outputpath where all (model) files are being saved
#
outputpath: "test_folder"  # If None, will be set to the file name (without extension)

#
# Description of the datasource used for 
# - training the tokenizer 
# - pre-training (for LM)
#
tokenizing and pre-training data source:
  filepath: "tokenizing_and_pre-training_data_file.txt"
  stripheader: False # if the custom data file has a header that has to be stripped
  columnsep: "\t" # could be ",", "|", "\t", ... Denominates the separator token of your file.
  tokensep: ","
  idpos: 1 # position of the identifier of the column 
  seqpos: 2 # position of the sequence column 
  pretrainedmodel: None

#
# Description of the fine-tuning source
#
fine-tuning data source:
  filepath: "fine-tuning_data_file.txt"
  stripheader: False # if the custom data file has a header that has to be stripped
  columnsep: "\t" # could be "," "|", "\t" ...
  tokensep: ","
  idpos: 1 # position of the identifier of the column 
  seqpos: 1 # position of the sequence column 
  labelpos: 1 # position of the label column 
  pretrainedmodel: None
  crossvalidation: False # trigger if cross-validation is desired. If set to `0`, no cross-validation is performed. If set to `True`, cross-validation is performed on the predifined splits in the data file, taking each split as a test set once. If set to an integer `x`, `x`-fold cross-validation is performed on random splits determined by `splitratio`.
  splitratio: False # Comma-seprated list describing the desired split ratio for train, validation and (possibly) test split for both cross-validation and non-cross-validation. Format is `train_percentage/val_percentage(/test_percentage)`, e.g. `85,15` or `70,20,10`. Must sum up to 100 (see default). Given a third splitratio triggers testing on that split. Will be overruled in case `splitpos` parameter is set.
  splitpos: False # int or `False` (if no splits are defined in the data file). Denotes the column in the data file where the split identifier is defined. If set to `True`, the split identifier is expected to be in the first column of the data file, i.e. the first column is expected to contain the split identifier. For non-cross-validation `devsplits` and `testsplits` must be set to use the splits.
  devsplits: False # a list, e.g. `[1, 2, ..]` to denote the splits that should be used for validation. `splitpos` must be set for this to work.
  testsplits: False # a list, e.g. `[1, 2, ..]` to denote the splits that should be used for validation. Setting this parameter will trigger testing on these splits. `splitpos` must be set for this to work.

#
# If you want to tokenize, you only need to specify the following.
#
tokenization:
  samplesize: None # if your data file is to big to learn a tokenizer, you can downsample it
  vocabsize: 20_000
  minfreq: 2
  atomicreplacements: None # dictionary of replacements from multi-char tokens to atomic characters, i.e. `{"-CDSstop-": "s"}
  maxtokenlength: 10
  encoding: bpe # DO NOT CHANGE. This is the default encoding for our XLNET models.
  lefttailing: False # If true, the sequences will be cut from the left (begging from the right end).

#
# The following are detailed settings for the training/fine-tuning mode. 
#
settings:
  data pre-processing:
    centertoken: False # either False or a token/character on which the sequence will be centered
  environment:
    ngpus: 1 # [1, 2, 4] # TODO: automatically infer this from the environment
  training:
    batchsize: 8
    gradacc: 4
    blocksize: 512 # DO NOT CHANGE. This is the default sequence length for our language models.
    nepochs: 10
    patience: 3
    resume: False # for resuming training
  fine-tuning:
    fromscratch: False # if we want to fine-tune without a pre-trained model (language models only)
    scaling: log # [log, minmax, standard]
    weightedregression: False

#
# Specific options for debugigng or dataset creation
#
debugging: 
  silent: False # If set to True, verbose printing of the transformers library is disabled. Only results are printed.
  dev: False # set to an integer if you want to run just on a fraction of the data to evaluate functionality of the scripts.
  getdata: False # if you only want to save your tokenized dataset and then quit